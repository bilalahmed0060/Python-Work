{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Large assignment 02.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": true,
        "id": "vkx3ZJypH3a5"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Read-before-you-start\" data-toc-modified-id=\"Read-before-you-start-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Read before you start</a></span></li></ul></li><li><span><a href=\"#Bayesian-Statistics:-Large-assignment-2\" data-toc-modified-id=\"Bayesian-Statistics:-Large-assignment-2-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Bayesian Statistics: Large assignment 2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-regression\" data-toc-modified-id=\"Linear-regression-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Linear regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Theft-in-Manhattan\" data-toc-modified-id=\"Theft-in-Manhattan-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Theft in Manhattan</a></span></li><li><span><a href=\"#One-or-two-lines?-A-regression-mixture-model\" data-toc-modified-id=\"One-or-two-lines?-A-regression-mixture-model-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>One or two lines? A regression mixture model</a></span></li><li><span><a href=\"#Game-of-Regression\" data-toc-modified-id=\"Game-of-Regression-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Game of Regression</a></span></li></ul></li><li><span><a href=\"#Comparing-models-for-small-images-data-using-logistic-regression\" data-toc-modified-id=\"Comparing-models-for-small-images-data-using-logistic-regression-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Comparing models for small images data using logistic regression</a></span></li></ul></li></ul></div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:25:53.958776Z",
          "start_time": "2020-11-11T20:25:53.163923Z"
        },
        "id": "KgcLdxwuH3bA"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "plt.rc('font', size=14)         # default fontsize \n",
        "plt.rc('axes', titlesize=16)    # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=16)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=14)   # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=14)   # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=16)   # legend fontsize\n",
        "plt.rc('figure', titlesize=16)  # fontsize of the figure title\n",
        "plt.rc('figure', titlesize=16)  # fontsize of the figure title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GttPElKqH3bA"
      },
      "source": [
        "# Bayesian Statistics: Large assignment 2\n",
        "\n",
        "Useful links:\n",
        "- [pyjags](https://pypi.org/project/pyjags/)\n",
        "- [JAGS](http://mcmc-jags.sourceforge.net)\n",
        "- [pyjags documentation](https://buildmedia.readthedocs.org/media/pdf/pyjags/stable/pyjags.pdf)\n",
        "- [ArviZ](https://arviz-devs.github.io/arviz/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:25:58.421442Z",
          "start_time": "2020-11-11T20:25:53.965916Z"
        },
        "id": "sl8D8tubH3bB"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "import pyjags\n",
        "import arviz as az"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEBut-XpH3bB"
      },
      "source": [
        "## Linear regression\n",
        "\n",
        "In this exercise you'll implement and experiment with a few linear regression models, as well as compare different variants.\n",
        "\n",
        "### Theft in Manhattan\n",
        "\n",
        "Let's take look at New York City crime data from 1966 to 1967. The outcome variable $y_i$ is the increase or decrease in the number of thefts in each $i$ of 23 Manhattan police precincts, compared to a baseline period earlier. The predictor $x_i$ is the percentage increase or decrease in the number of police officers in a precinct. Together with some hyperparameter settings, this forms the following model:\n",
        "\n",
        "\\begin{align*}\n",
        "    \\tau &\\sim \\text{Gamma}(0.01, 0.01) \\\\\n",
        "    w_j &\\sim \\text{Gaussian}(0,1.0)& j=0,1\\\\\n",
        "    \\mu_i \\mid w_0, w_1, x_i &= w_0 + w_1x_i& i=1\\ldots n \\\\\n",
        "    y_i \\mid\\mu_i, \\tau & \\sim \\text{Gaussian}(\\mu_i, \\tau)& i=1\\ldots n\n",
        "\\end{align*} Note: we use the JAGS convention to specify the variance of the Gaussian distribution, with $\\tau=1/\\sigma^2$. This means that large values of $\\tau$ generate data points very close to $\\mu_i$, while small values generate data points that may deviate further. This way, our notation here should match the notation in JAGS. We call $\\tau$ the *precision* of the Gaussian.\n",
        "\n",
        "**1.** Implement this linear regression model in pyjags.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:25:58.440052Z",
          "start_time": "2020-11-11T20:25:58.425336Z"
        },
        "id": "vCEzZLVZH3bC"
      },
      "source": [
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLZHIQwrH3bC"
      },
      "source": [
        "---\n",
        "\n",
        "**2.** The data (the predictors $x_i$ and the to-be-predicted values $y_i$) are available below. Use this data to learn the posterior distribution $p(w_0, w_1\\mid \\mathbf{x},\\mathbf{y})$. Plot histograms that approximate the posterior densities of both. Does adding more police staff increase or decrease the amount of thefts?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:26:00.074694Z",
          "start_time": "2020-11-11T20:25:58.489035Z"
        },
        "id": "d8cVdUYTH3bC"
      },
      "source": [
        "x = np.array([-15.79, 0.98, 3.71, -5.37, -10.23, -8.32, -7.80, 6.77, -8.81, -9.56, -2.06, -0.76, -6.30, 39.40, \\\n",
        "     -10.79, -8.16, -2.82, -16.19, -11.00, -14.60, -17.96, 0.76, -10.77])\n",
        "y = np.array([3.19, -3.45, 0.04, 6.62, 3.61, 2.67, -2.45, 9.31, 15.29, 3.68, 8.63, 10.82, -0.50, -11.00, 2.05, \\\n",
        "     -11.80, -2.02, 4.42, -0.86, -0.92, 2.16, 1.58, -1.16])\n",
        "n = len(y)\n",
        "\n",
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEz8VVFiH3bD"
      },
      "source": [
        "<span style=\"color:red\">\n",
        "\n",
        "Your answer here.\n",
        "    \n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W17j1OCzH3bD"
      },
      "source": [
        "---\n",
        "\n",
        "**3.** Make a new plot in which you show the data points as individual points. On top of that, plot the line that $w_0$ and $w_1$ define. Use the expectation of the approximated posterior for these weights, i.e. the mean of the sampled values.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:26:01.326674Z",
          "start_time": "2020-11-11T20:26:00.081361Z"
        },
        "id": "zuv_ECWfH3bD"
      },
      "source": [
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JNqUkC4H3bE"
      },
      "source": [
        "<span style=\"color:red\">\n",
        "\n",
        "Your answer here.\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEueGgOeH3bE"
      },
      "source": [
        "---\n",
        "\n",
        "**4.** Classical statistical inference would stop here. However, in Bayesian inference we don't actually have *just* the expectation of the posterior, but the whole distribution! This allows us to characterize the *uncertainty* in our estimates of this linear relationship. To visualize this copy the plot you made previously, and superimpose 500 *sampled* lines from the posterior. To do so, you need to randomly select 500 samples from the ones that you have collected using pyjags, then from each sample obtain $w_0$ and $w_1$, and then plot the line. To improve visibility, make the sample lines transparent. Multiple transparent lines will create opaque parts where the estimate is most certain (as all samples go through that area). Using this visualization, where is the model most certain about its predictions? And where is it most uncertain? Why is it more uncertain there, compared to the other parts of the plot?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:26:04.279616Z",
          "start_time": "2020-11-11T20:26:01.331922Z"
        },
        "id": "2yV6l5kWH3bE"
      },
      "source": [
        "import random\n",
        "\n",
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAGcZtFjH3bE"
      },
      "source": [
        "<span style=\"color:red\">\n",
        "\n",
        "Your answer here.\n",
        "    \n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwwmVfM3H3bF"
      },
      "source": [
        "---\n",
        "\n",
        "### One or two lines? A regression mixture model\n",
        "\n",
        "Consider again a seemingly simple linear regression task with only one predictor $x_i$ and one prediction $y_i$ for every observation (= data point) $i$.\n",
        "\n",
        "**1.** Apply your model from the previous exercise to the data below and learn the posterior distribution $p(w_0, w_1\\mid \\mathbf{x},\\mathbf{y})$ using this new data set. Plot the data points, the expectation of the predicted line (using the expectations of $w_0$ and $w_1$), and 500 samples from the posterior. Where in the figure does the fitted line fit the data best, i.e. where is the uncertainty lowest?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:26:04.298710Z",
          "start_time": "2020-11-11T20:26:04.286563Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "-X1cnn4vH3bF",
        "outputId": "e489c94f-ba15-4d63-cac0-85af1eaf0217"
      },
      "source": [
        "x = np.loadtxt('twolines/x.csv', delimiter=',')\n",
        "y = np.loadtxt('twolines/y.csv', delimiter=',')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-af6409d38ca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'twolines/x.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'twolines/y.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-bDlGiqH3bF"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:26:08.013141Z",
          "start_time": "2020-11-11T20:26:04.311594Z"
        },
        "id": "7qPUlP83H3bF"
      },
      "source": [
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX9KEZwKH3bG"
      },
      "source": [
        "<span style=\"color:red\">\n",
        "\n",
        "Your answer here.\n",
        "    \n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zv6iFb0H3bG"
      },
      "source": [
        "---\n",
        "\n",
        "**2.** The previous question asked you to blindly apply a simple linear model to data. But as you can see now that you have plotted the data and the estimated line, this data actually appears to come from *two* lines instead of one (that is, each data point belongs to one of the two lines). Extend the generative model to describe that each data point $(x_i, y_i)$ comes from *one of two* lines (each with its own intercept and slope). You will need an additional variable $z_i$ that indicates from which line data point $i$ comes. You may assume that each line is equally likely in your prior on $z_i$. (Hint: This is a mixture model, so you can take inspiration from mixture models you have seen before!)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j7Vq1fEH3bG"
      },
      "source": [
        "<span style=\"color:red\">\n",
        "\n",
        "Your answer here.\n",
        "    \n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT-5iPy4H3bG"
      },
      "source": [
        "---\n",
        "\n",
        "**3.)** Implement the extended model in pyjags and run it using the data. Plot the data points like you did before, and superimpose 500 samples from this new model. Note that a single sample consists of the parameters for two lines (and the precision). To \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:26:37.959401Z",
          "start_time": "2020-11-11T20:26:08.059704Z"
        },
        "id": "hUfbk2PmH3bG"
      },
      "source": [
        "# Your answer here - the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:26:40.750649Z",
          "start_time": "2020-11-11T20:26:37.964826Z"
        },
        "id": "DeduZuDYH3bG"
      },
      "source": [
        "# Your answer here - the plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gSYSf86H3bH"
      },
      "source": [
        "<span style=\"color:red\">\n",
        "\n",
        "Your answer here.\n",
        "    \n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_XvJw04H3bH"
      },
      "source": [
        "---\n",
        "\n",
        "**4.** Plot, in the same figure, the *expectation* of the two estimated lines. Does the expectation match your expectation?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:26:45.596797Z",
          "start_time": "2020-11-11T20:26:40.753664Z"
        },
        "id": "CNT4SSPgH3bH"
      },
      "source": [
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01noI-GFH3bH"
      },
      "source": [
        "<span style=\"color:red\">\n",
        "\n",
        "Your answer here.\n",
        "    \n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DRdzbwpH3bH"
      },
      "source": [
        "---\n",
        "\n",
        "**5.** Given your plots, it (hopefully) seems that the model with two regression lines explains the data much better than the model with one line. But we want to make sure! Implement a model comparison model in pyjags, using an additional categorical variable $m$. When $m=1$, the model should explain the data using just one line, while when $m=2$, the two regression lines should be used. The two models may use the same precision parameter $\\tau$. To make sure you implemented everything correctly, plot again the data, and then plot 250 samples for $m=1$ and 250 samples for $m=2$. Note that the latter samples again contain two lines. Use different colors for samples from each of the two models.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:27:23.506276Z",
          "start_time": "2020-11-11T20:26:45.599460Z"
        },
        "id": "CG7TqrkYH3bH"
      },
      "source": [
        "# Your answer here - the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:27:26.011127Z",
          "start_time": "2020-11-11T20:27:23.508481Z"
        },
        "id": "52KNX8f9H3bI"
      },
      "source": [
        "# Your answer here - the plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d5B2BmWH3bI"
      },
      "source": [
        "<span style=\"color:red\">\n",
        "\n",
        "Your answer here\n",
        "    \n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crFVnPcFH3bI"
      },
      "source": [
        "---\n",
        "\n",
        "**6.** Run this model on the data, and compute (and show) the posterior *model* probabilities $p(m_1 \\mid \\mathbf{x}, \\mathbf{y})$ and $p(m_2\\mid \\mathbf{x}, \\mathbf{y})$, as well as the Bayes factor. Look up the [Bayes factor interpretation table](https://en.wikipedia.org/wiki/Bayes_factor) on Wikipedia. Do the outcomes match your intuition? According to this table, can you conclude that the model with 2 lines is better?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:27:26.140016Z",
          "start_time": "2020-11-11T20:27:26.013603Z"
        },
        "id": "X9Vj3HKAH3bI"
      },
      "source": [
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf3gOUMfH3bI"
      },
      "source": [
        "<span style=\"color:red\">\n",
        "\n",
        "Your answer here.\n",
        "    \n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGEMOIHJH3bI"
      },
      "source": [
        "---\n",
        "\n",
        "### Game of Regression\n",
        "\n",
        "*This exercise was conceived when the last season of Game of Thrones had just started. Today, we'll just have to imagine we haven't seen the last season yet. Lucky you. Anyway, back to Bayes.*\n",
        "\n",
        "The new (and final) season of Game of Thrones has started, and we are all wondering which characters will survive and who will perish. We will answer this question using Bayesian linear regression. The goal is to to predict the age (at death) of the characters. Let $y_i$ be the age that character $i$ dies at, and:\n",
        "- $x_{i,1}$ is the number of pages in the books on which $i$ is mentioned,\n",
        "- $x_{i,2}$ is the number of weddings $i$ attends,\n",
        "- $x_{i,3}$ is the size of household $i$ is from,\n",
        "- $x_{i,4}$ is the length of $i$ in centimeters and finally\n",
        "- $x_{i,5}$ is the number of times $i$ has said 'winter is coming'.\n",
        "\n",
        "We will use the same model as previously, with one change: we now have multiple predictors. That means the model becomes\n",
        "\\begin{align*}\n",
        "    \\tau &\\sim \\text{Gamma}(0.01, 0.01) \\\\\n",
        "    w_0 &\\sim \\text{Gaussian}(0,1.0)\\\\\n",
        "    w_j &\\sim \\text{Gaussian}(0,1.0)& j=1\\ldots p\\\\\n",
        "    \\mu_i \\mid w_0, \\mathbf{w} &= w_0 + \\sum_{j=1}^p w_jx_{ij} = w_0+\\mathbf{w}^T \\mathbf{x}_i& i=1\\ldots n\\\\\n",
        "    y_i \\mid| \\mu_i, \\tau & \\sim \\text{Gaussian}(\\mu_i, \\tau)& i=1\\ldots n \\enspace.\n",
        "\\end{align*} \n",
        "\n",
        "Data is available for this exercise in a separate file for $\\mathbf{x}$ and $y$. Load the data using the code below. Note that $\\mathbf{x}$ is a *matrix* where each *row* contains the values for the $p=5$ predictors for one character. There are data available for $n=100$ characters (so essentially we have 100 observations to base our estimates of $\\mathbf{w}$ on). The ages $\\mathbf{y}$ are stored in a vector, with each element $y_i$ being the age at which the corresponding character $i$ died."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:27:26.153957Z",
          "start_time": "2020-11-11T20:27:26.144872Z"
        },
        "id": "asVWjEFnH3bJ"
      },
      "source": [
        "x = np.loadtxt('gameofregression/x.csv', delimiter=',')\n",
        "y = np.loadtxt('gameofregression/y.csv', delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iELBVztH3bJ"
      },
      "source": [
        "**1.** Implement the linear regression model in pyjags and learn the posterior distributions for each element of $\\mathbf{w}$, as well as for $w_0$. (Tip: it will be easier to implement $w_0$ as a separate parameter than as an element of the vector $\\mathbf{w}$.) Try using the linear algebra notation $\\mathbf{w}^T \\mathbf{x}_i$ (i.e. the inner product) instead of summing over each of the predictors manually. Plot the histograms of the approximated posterior distribution of each of these weights. One of these predictors hardly seems to influence the age of death. Which one is that?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:27:30.928292Z",
          "start_time": "2020-11-11T20:27:26.162515Z"
        },
        "id": "_5hpKXKVH3bJ"
      },
      "source": [
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lssXRLVYH3bJ"
      },
      "source": [
        "<span style=\"color:red\">\n",
        "\n",
        "Your answer here.\n",
        "   \n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-I_bX-BH3bJ"
      },
      "source": [
        "---\n",
        "\n",
        "**2.** Imagine: In the fifth episode of season 8, George R.R. Martin introduces a new character. The character has the following predictor vector  $\\mathbf{x}_{n+1} = (1, 0, 5, 184, 20)$. What is the predicted age of death? Had the character been a giant instead, 380cm tall, what would the predicted age of death be?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:27:31.277996Z",
          "start_time": "2020-11-11T20:27:30.932345Z"
        },
        "id": "-NtUorYDH3bJ"
      },
      "source": [
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34ynpb2LH3bK"
      },
      "source": [
        "<span style=\"color:red\">\n",
        "\n",
        "Your answer here.\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af3jfuPAH3bK"
      },
      "source": [
        "---\n",
        "\n",
        "## Comparing models for small images data using logistic regression\n",
        "\n",
        "In this exercise you will examine model comparison for a small, but conceptually realistic case, where it is possible to enumerate all possible *data sets*. (Not all parameters, but all data sets. See the knowledge clip on Occam's razor for more on this.) By computing the evidence for each data set, you will see that some models fit more possible data sets than other models would, but that there is not a single model that always 'wins'. For this exercise, there will be a bit more programming in Python, but we will not use pyjags.\n",
        "\n",
        "![](images/pixelgrids.pdf)\n",
        "\n",
        "Consider a very simple agent (for example, a robot) that has a camera with a resolution of only 9 pixels. These pixels are binary, i.e. they can only be on or off. They are arranged in a square grid, so that each pixel has the following coordinates (see also (a)):\n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathbf{x}^{(1}) &= (-1, +1), & \\mathbf{x}^{(2)} &= (0, +1), & \\mathbf{x}^{(3)} &= (+1, +1), \\\\\n",
        "    \\mathbf{x}^{(4)} &= (-1, 0), & \\mathbf{x}^{(5)} &= (0, 0), & \\mathbf{x}^{(6)} &= (+1, 0), \\\\\n",
        "    \\mathbf{x}^{(7)} &= (-1, -1), & \\mathbf{x}^{(8)} &= (0, -1), & \\mathbf{x}^{(9)} &= (+1, -1)\\enspace.\n",
        "\\end{align*} \n",
        "\n",
        "Each pixel $\\mathbf{x}^{(i)}$ is a vector of two coordinates, $x^{(i)}_1$ and $x^{(i)}_2$. The superscript ${(i)}$ indicates the number of the pixel. Whether a pixel is on (value $+1$) or off (value $-1$), is indicated by another vector containing the labels $\\{y^{(i)}\\}^9_{i=1}$, $y^{(i)}\\in\\{-1,1\\}$. (Note the slightly different convention of -1 and +1 here, compared to 0 and 1.) A single data set $D$ is a combination of the pixels (which are the same for all data sets, they are just these 9 pixels) and the vector of labels, so we have $D=(\\mathbf{x}, \\mathbf{y})$. A few example data sets are shown in figures (b-d).\n",
        "\n",
        "The task for the robot is to determine a *straight line* (known as a *decision boundary*) that separates the black from the white pixels. It does so by learning the parameters $\\mathbf{w}$ that describe this line. Today however, we are not interested in the values of these weights, but rather in which *model* the robot uses to learn these. We'll compare four models, each with the same prior, but with different likelihood functions. Note that these are not the familiar beta, Gaussian or Bernoulli distributions, but don't be intimidated - they are just functions describing the probability of data, given parameters and a choice for a model:\n",
        "\n",
        "\\begin{align}\n",
        "  p(D\\mid \\mathbf{w}, m_0) &= \\frac{1}{512} \\label{eq:model1}\\\\\n",
        "  p(D\\mid \\mathbf{w}, m_1) &= \\prod_{i=1}^9 \\frac{1}{1+ e^{-y^{(i)} w_1 x_1^{(i)}}} \\\\\n",
        "  p(D\\mid \\mathbf{w}, m_2) &= \\prod_{i=1}^9 \\frac{1}{1+ e^{-y^{(i)} \\left(w_1 x_1^{(i)} + w_2 x_2^{(i)}\\right)}} \\\\\n",
        "  p(D\\mid \\mathbf{w}, m_3) &= \\prod_{i=1}^9 \\frac{1}{1+ e^{-y^{(i)} \\left(w_0 + w_1 x_1^{(i)} + w_2 x_2^{(i)}\\right)}} \\enspace.\n",
        "\\end{align} \n",
        "\n",
        "As you can see, the models become increasingly complex:\n",
        "1. The first model generates data uniformly and doesn't even use the pixel information. Its weight vector $\\mathbf{w}$ is empty; there are no parameters to learn. This means that all 512 possible 3-by-3 `images' are equally likely.\n",
        "2. The second model takes into account only the first coordinate (the horizontal direction) for each of the nine pixels. The strength of this coordinate on the probability of the data is modulated by a weight, $w_1$. In this model, decision boundaries are always vertical lines.\n",
        "3. The third model takes both coordinates into account, modulated by a weight for each, $w_1$ and $w_2$. Lines can now have any orientation.\n",
        "4. The fourth model also includes an *offset* weight $w_0$. Now the decision boundaries don't have to cross the (0,0) pixel anymore.\n",
        "\n",
        "The latter three models are examples of *logistic regression*: the binary pixel status $y^{(i)}$ (on or off) is predicted using the pixel coordinate $\\mathbf{x}^{(i)}$.\n",
        "\n",
        "Figures (b-d) show some examples of different data sets. In the figure above, you see that a pixel being on or off only depends on coordinate $x_1$: pixels to the right of the grid are on, all other pixels are off. The data set in (c) shows a dependency on both coordinates (i.e. the decision boundary is be a diagonal line). The data set in (d) is not easy to separate into black and white pixels using a straight line. None of the models will fit perfectly here. Finally, data set (e) is another example of a diagonal decision boundary. However, this boundary does not go through the middle of the pixel grid (i.e. not through the (0,0) pixel). That means we need an *offset* that shifts this line up or down. Examples of decision boundaries are shown with thick red lines.\n",
        "\n",
        "Each model uses the same prior distribution on the weights (of course, only for the weights that the model actually uses):\n",
        "\n",
        "\\begin{equation*}\n",
        "    p(w_j \\mid m_i) = \\text{Gaussian}(\\mu=0, \\sigma^2=100) \\quad\\forall_{i,j} \\enspace,\n",
        "\\end{equation*} \n",
        "\n",
        "i.e. a Gaussian distribution with mean $\\mu = 0$ and variance $\\sigma=10$ (note: $\\sigma=10 \\leftrightarrow \\sigma^2=100$, in Python you need to supply $\\sigma$ to `np.random.normal`).\n",
        "\n",
        "For any given data set $D$, one might learn the posterior $p(\\mathbf{w}\\mid D)$, i.e. which weights (and hence, which decision boundary) best describes the data. Here however, we are interested in which *model* works best - regardless of its specific parameter values! You will do this by computing the (approximate) *evidence* for each of the 512 data sets, using Monte Carlo.\n",
        "\n",
        "Step one is to enumerate and store all the 512 data sets. This is already done for you. The data sets can be loaded with the code below. Your task is to compute the model evidences for each model and each data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:27:31.301383Z",
          "start_time": "2020-11-11T20:27:31.286014Z"
        },
        "id": "MIThKOd2H3bM"
      },
      "source": [
        "datasets = np.loadtxt('images/datasets.csv', delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mlQEG--H3bN"
      },
      "source": [
        "**1.**  Start with computing the evidence $p(D\\mid m_0)$, which you can do exactly using the equation above and the fact that there are no weights in $m_0$.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T20:27:31.312557Z",
          "start_time": "2020-11-11T20:27:31.305237Z"
        },
        "id": "bAj-29sGH3bN"
      },
      "source": [
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxD8PbFuH3bN"
      },
      "source": [
        "<span style=\"color:red\">\n",
        "\n",
        "Your answer here.\n",
        "    \n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyNPEQsWH3bO"
      },
      "source": [
        "---\n",
        "\n",
        "**2.** Computing the evidence for the other models ($m_1, m_2, m_3$) cannot be done analytically. But we can do this approximately using Monte Carlo, i.e. by drawing *samples*. See book/slides for how to do this. In short:\n",
        "1. Draw sample weights $\\mathbf{w}$ from the prior, i.e. $\\mathbf{w}^{(s)}\\sim \\text{Gaussian}(\\mu,\\sigma^2)$. Make sure you have the number of weights correct, as the different models have different numbers of weights.\n",
        "2. Using those weights, compute $p(D\\mid\\mathbf{w}^{(s)}, m_i)$.\n",
        "\n",
        "Repeat this process $S$ times, so you collect $S$ weight sampled from the prior, and for each you compute the likelihood. The average of these likelihoods approximates the evidence, that is:\n",
        "        \\begin{equation}\n",
        "            p(D\\mid m_i) \\approx \\frac{1}{S}\\sum_{w^{(s)}\\sim \\text{Gaussian}(\\mu,\\sigma^2)}^S p(D\\mid\\mathbf{w}^{(s)}, m_i)\n",
        "        \\end{equation} approximates the evidence for model $m_i$. Compute this approximation for every data set, and for each of the models $m_1, m_2, m_3$, using $S=10\\,000$ samples. Running the computation may take a few minutes, since you are computing the likelihood $3\\times 512\\times 10\\,000$ times!\n",
        "        \n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-12T08:25:51.424227Z",
          "start_time": "2020-11-11T20:27:31.316922Z"
        },
        "id": "5uYBImUyH3bO"
      },
      "source": [
        "def model1likelihood(x, y, w):\n",
        "    # ...\n",
        "    return None\n",
        "\n",
        "def model2likelihood(x, y, w):\n",
        "    # ...\n",
        "    return None\n",
        "\n",
        "def model3likelihood(x, y, w):\n",
        "    # ...\n",
        "    return None\n",
        "\n",
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5ICAZ4aH3bO"
      },
      "source": [
        "<span style=\"color:red\">\n",
        "\n",
        "Your answer here.\n",
        "    \n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWSfmA4KH3bO"
      },
      "source": [
        "---\n",
        "\n",
        "**3.** Now that you have computed the evidence for all data sets and all models, use the available plotting function to visualize the result. Sort the output by $m_3$. Two data sets are *much* better explained by $m_3$ than by any of the other models. Which data sets are these? Why can they only be reasonably explained by $m_3$? What is the Bayes factor between $m_0$ and $m_3$ for this data set?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-12T08:25:52.643783Z",
          "start_time": "2020-11-12T08:25:51.488069Z"
        },
        "id": "l7YoKbPsH3bO"
      },
      "source": [
        "def plot_evidences(evidences, sortBy):\n",
        "    idxs_sorted = np.flip(evidences[:,sortBy].argsort())\n",
        "    plt.figure(figsize=(15,6))\n",
        "    plt.plot(range(len(datasets)), evidences[idxs_sorted], '.')\n",
        "    plt.title('Model evidences for each dataset')\n",
        "    plt.xlabel('Dataset number')\n",
        "    plt.ylabel('Evidence') \n",
        "    plt.legend(['Model 0', 'Model 1', 'Model 2', 'Model 3'])\n",
        "\n",
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SZLf67SH3bO"
      },
      "source": [
        "<span style=\"color:red\">\n",
        "\n",
        "Your answer here.\n",
        "    \n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm5hMe3iH3bO"
      },
      "source": [
        "---\n",
        "\n",
        "**4.** You can argue that $m_3$ is a more complex model than $m_0$. After all, $m_0$ doesn't have any free parameters, while $m_3$ has $w_0, w_1$ and $w_2$, and hence, much more freedom to fit to complex structure in the data. However, additional complexity comes at a cost (see the slides on Occam's Razor). For how many of the data sets is $m_0$ actually favored over $m_3$? Compute the same for all $4\\times 4$ model comparisons. Which model seems to best describe the majority of possible data sets?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-12T08:25:52.706833Z",
          "start_time": "2020-11-12T08:25:52.659313Z"
        },
        "id": "SadnNZpzH3bP"
      },
      "source": [
        "# Your answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyG5S6NAH3bP"
      },
      "source": [
        "<span style=\"color:red\">\n",
        "\n",
        "Your answer here.\n",
        "    \n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqg5cd9jH3bP"
      },
      "source": [
        "---\n",
        "\n",
        "**5.** Despite the clear dominance of that model, why might the (designers of the) robot still prefer one of the other models if the robot was to, say, use its model to navigate through a forest containing only very tall and straight trees?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNGPc2XfH3bP"
      },
      "source": [
        "<span style=\"color:red\">\n",
        "\n",
        "Your answer here.\n",
        "    \n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8MPCMmCH3bP"
      },
      "source": [
        "---"
      ]
    }
  ]
}