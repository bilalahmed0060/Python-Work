{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Predicting sentiment from product reviews\n",
                "\n",
                "The goal of this first notebook is to explore logistic regression and feature engineering with sklearn.\n",
                "\n",
                "In this notebook you will use product review data from Amazon.com to predict whether the sentiments about a product (from its reviews) are positive or negative.\n",
                "\n",
                "* Use Pandas Dataframes to do feature engineering\n",
                "* Train a logistic regression model to predict the sentiment of product reviews.\n",
                "* Inspect the weights (coefficients) of a trained logistic regression model.\n",
                "* Make a prediction (both class and probability) of sentiment for a new product review.\n",
                "* Given a classifier, create a confusion matrix\n",
                "* Compare multiple logistic regression models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "import string\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "from sklearn.feature_extraction import DictVectorizer\n",
                "\n",
                "sns.set()\n",
                "%matplotlib inline\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data preparation\n",
                "\n",
                "We will use a dataset consisting of food product reviews on Amazon.com [source](http://jmcauley.ucsd.edu/data/amazon/)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "products = pd.read_csv('food_products.csv')\n",
                "\n",
                "products"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extract sentiments\n",
                "\n",
                "We will **ignore** all reviews with *rating = 3*, since they tend to have a neutral sentiment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "products = products[products['rating'] != 3]\n",
                "products = products.copy()  # This is to avoid having a view on the old data\n",
                "\n",
                "len(products)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.title('Number of reviews with a given rating')\n",
                "sns.histplot(products['rating'])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we will assign reviews with a rating of 4 or higher to be *positive* reviews, while the ones with rating of 2 or lower are *negative*. For the sentiment column, we use +1 for the positive class label and -1 for the negative class label."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "products['sentiment'] = products['rating'].apply(lambda rating : +1 if rating \u003e 3 else -1)\n",
                "products.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we can see that the dataset contains an extra column called **sentiment** which is either positive (+1) or negative (-1)."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Build the word count vector for each review"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let us explore a specific example of a food product. We have information about the product, the review left, and both the rating that was given and the sentiment label we computed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "products.iloc[21]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To work with the text data, we will need to eventually turn it into word counts. In other words, we will be making a feature for every word that could possibly appear in the data, and the value for that feature for one example would be the number of times that word appears in that example. \n",
                "\n",
                "To accomplish this, we will need to do two data transformation:\n",
                "\n",
                "1. Remove punctuation using [Python's built-in](https://docs.python.org/2/library/string.html) string functionality.\n",
                "2. Transform the reviews into word-counts.\n",
                "\n",
                "\n",
                "\n",
                "\u003e **Aside**. In this assignment, we remove all punctuations for the sake of simplicity. A smarter approach to punctuations would preserve phrases such as \"I'd\", \"would've\", \"hadn't\" and so forth. \n",
                "\u003e \n",
                "\u003e If you are curious in learning how to handle these complexities in practice, you might be interested in  researching more about tokenization and NLP like [this page](https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4) shows. Note that you do not need to do any of that stuff for this assignment.\n",
                "\n",
                "So first, we remove punctuation with the code in the next cell."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def remove_punctuation(text):\n",
                "    if type(text) is str:\n",
                "        return text.translate(str.maketrans('', '', string.punctuation))\n",
                "    else:\n",
                "        return ''\n",
                "    \n",
                "products['review_clean'] = products['review'].apply(remove_punctuation)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we use scikit-learn's [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to get counts for each word. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make counts\n",
                "vectorizer = CountVectorizer()\n",
                "count_matrix = vectorizer.fit_transform(products['review_clean'])\n",
                "\n",
                "# Make a new DataFrame with the counts information\n",
                "product_data = pd.DataFrame(count_matrix.toarray(),\n",
                "        index=products.index,\n",
                "        columns=vectorizer.get_feature_names())\n",
                "\n",
                "# Add the old columns to our new DataFrame. \n",
                "# We won't use review_clean and the summary in our model, but we will keep\n",
                "# them to look at later.\n",
                "product_data['sentiment'] = products['sentiment']\n",
                "product_data['review_clean'] = products['review_clean']  \n",
                "product_data['summary'] = products['summary']\n",
                "\n",
                "product_data.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We have now created a lot of features to work with! Note that in the table above, we will have one feature for each word taht appeared in the data and the value for that feature is the count of that word in that review. So for example, if review 5 had the word \"dog\" in it 3 times, the value in row 5 and column \"dog\" would be 3.\n",
                "\n",
                "## Split data into training, validation and test sets."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's perform a train/validation/test split with 80% of the data in the training set, 10% of the data in the validation set, 10% test. Note that we use `random_state=3` so that everyone gets the same result."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_data, test_data = train_test_split(product_data, test_size=0.2, random_state=3)\n",
                "validation_data, test_data = train_test_split(test_data, test_size=0.5, random_state=3)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Baseline: Majority class prediction\n",
                "\n",
                "It is quite common to use the **majority class classifier** as the a baseline (or reference) model for comparison with your classifier model. The majority classifier model predicts the majority class for all data points.\n",
                "\n",
                "To \"train\" the majority class classifier, you should simply find the most frequent target in the training data.\n",
                "\n",
                "### **Question 1:** Majority class classifier\n",
                "* Compute the most frequent label and store it in a variable called `majority_label`.\n",
                "* What is the validation accuracy of the majority class classifer. Store your result as a number between 0 and 1 in a variable called `majority_classifier_validation_accuracy`.\n",
                "  \n",
                "  *Hint:* pandas allows you to take the sum of a boolean series - true values are equal to 1, false values are equal 0."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q1_majority_classifier) ###\n",
                "\n",
                "# TODO \"Train\" a majority class classifier and calculate its validation accuracy\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Train a sentiment classifier with logistic regression\n",
                "\n",
                "We will now use logistic regression to create a sentiment classifier on the training data. This model will use the columns representing word coutnts as features and the column **sentiment** as the target. We will set **no regularization penalty** (and set `random_state=1` to get the same answer as everyone else). \n",
                "\n",
                "You can see scikit-learn's documentation for LogisticRegression [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Note that the parameter for this class to control regularization is named `C` and it represents the inverse of the penalty strenght. In other words $C = \\frac{1}{\\lambda}$. This means to have very little regularization, we make `C` a very large number (corresponding to a very small $\\lambda$)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "features = vectorizer.get_feature_names()\n",
                "\n",
                "# Note: C = 1/Lambda. Setting C to a really high value is the same as setting lambda = 0\n",
                "sentiment_model = LogisticRegression(penalty='l2', random_state=1, C=1e23)\n",
                "sentiment_model.fit(train_data[features], train_data['sentiment'])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's look at some of the coefficients and the corresponding words. The weights are stored in a `coef_` property: "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "coefficients = sentiment_model.coef_[0] \n",
                "\n",
                "print('Smallest coefficient', coefficients.min())\n",
                "print('Largest coefficient:', coefficients.max())"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **Question 2:** Most Positive/Negative Word\n",
                "For the sentiment model we trained above, compute the word with the most negative weight and the word with the most positive weight.\n",
                "\n",
                "Store your results in the variables `most_negative_word` and `most_positive_word`.\n",
                "\n",
                "While you only need to write code to compute the most negative and most positive, we also recommend printing out the words with the highest magnitude coefficients to make sure they make sense.\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q2_most_pos_neg_words) ###\n",
                "\n",
                "# TODO Find the most positive word and most negative word in the sentiment_model"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Making predictions with logistic regression\n",
                "\n",
                "Now that a model is trained, we can make predictions on the **validation data**. In this first section, we will restrict the examples we are looking at to 3 examples in the validation dataset. We refer to this set of 3 examples as the **sample_data**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "sample_data = validation_data[8:11]\n",
                "sample_data[['sentiment', 'review_clean', 'summary']]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Predicting sentiment\n",
                "Let's start by predicting the sentiment of the 3 examples in the `sample_data`. The `predict_proba` method on the `LogisticRegression` class outputs a probability for each class possible.\n",
                "\n",
                "The output has one row for each example. Each row is an array of 2 numbers, the first is the predictor's prediction for the probability it is a negative sentiment example, and the second is the probability of it being a positive sentiment example."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('  Prob Negative, Prob Positive')\n",
                "print(sentiment_model.predict_proba(sample_data[features]))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We are also able to make predictions using the `predict` function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Predicted labels')\n",
                "print(sentiment_model.predict(sample_data[features]))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **Question 3:** Find the most positive (and negative) review"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now turn to examining the full **validation_data** dataset \u003cspan style=\"color:red\"\u003e(not sample_data)\u003c/span\u003e, , and use `sklearn` to form predictions on all of the data points for faster performance.\n",
                "\n",
                "Using the `sentiment_model`, find review in the **validation_data** with the **highest probability** of being classified as a **positive review**. Also, find the reivew with the **highest probability** of being classified as a **negative review**. We refer to these as the \"most positive review\" and \"most negative review\" respectively. Store the `review_clean` column value for each of these rows in `most_positive_review` and `most_negative_review` respectively.\n",
                "\n",
                "If there is a tie for the most positive/negative reivew, you should always grab the one that appears *first* in the validation data.\n",
                "\n",
                "*Hint*: Once you know the index of the most positive/negative reviews, use the `.iloc[]` accessor on the DataFrame to get that row and find its `review_clean` value."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q3_most_positive_negative_review) ###\n",
                "\n",
                "# TODO Find the review_clean values for the most positive and most negative review"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Do you notice something special about those reviews? They are both pretty long! Here we just count number of words regardless of the length of the review but clearly that can affect the results, and in practice one can use some techniques to normalize the counts to avoid prioritizing long reviews over shorter ones (we will discuss this idea in a future week).\n",
                "\n",
                "### **Question 4:** Compute validation accuracy\n",
                "Compute the validation accuracy for the model we just trained. Report the validationaccuracy as a number between 0 and 1 stored in a variable called `sentiment_model_validation_accuracy`.\n",
                "\n",
                "Below, calculate the accuracy of the predictor using sklearn's [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q4_sentiment_model_accuracy) ###\n",
                "\n",
                "# TODO Find the validation accuracy of the sentiment model"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **Question 5**: Which model?\n",
                "Compare the validation accuracy for this problem with the validation accuracy of the baseline majority class classifier. Which model would you predict will peform better in the future? \n",
                "\n",
                "* If you think the majority class classifier would do better, write `q5 = 'majority_class_classifier'`.\n",
                "* If you think the sentiment model would do better, write `q5 = 'sentiment_model'`.\n",
                "* If you think we can't choose which model will be best yet since we haven't assessed on the test set, write `q5 = 'cannot say'`.\n",
                "\n",
                "Save your variable indiciating your answer in the next cell."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q5_which_model) ###\n",
                "\n",
                "# TODO Answer the question posed above.\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Create a confusion matrix\n",
                "\n",
                "A common tool used when analyzing the peformance of a predictor in a classification problem is to look at the confusion matrix, as well as the overall accuracy.\n",
                "\n",
                "We've created a function that will plot a confusion matrix for you given a set of inputs which are the values that should appear within each cell.\n",
                "Recall that there are four values associated with a confusion matrix: true positive, true negative, false positive, and false negative which we will abberviate as TP, TN, FP, and FN, respecitvely. In other words, for the next problem we have handled the plotting code for you that you can use, but you will need to compute the values for each of the confusion matrix dimensions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_confusion_matrix(tp, fp, fn, tn):\n",
                "    \"\"\"\n",
                "    Plots a confusion matrix using the values \n",
                "       tp - True Positive\n",
                "       fp - False Positive\n",
                "       fn - False Negative\n",
                "       tn - True Negative\n",
                "    \"\"\"\n",
                "    data = np.matrix([[tp, fp], [fn, tn]])\n",
                "\n",
                "    sns.heatmap(data,annot=True,xticklabels=['Actual Pos', 'Actual Neg']\n",
                "              ,yticklabels=['Pred. Pos', 'Pred. Neg']) "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **Question 6:** Compute confusion matrix values and plot\n",
                "\n",
                "Write code below that uses the `plot_confusion_matrix` function to show the number of true positive, true negative, false positive, and false negative predictions made by your classifier. You should store the counts for each of these values in the variables:\n",
                "* `tp`\n",
                "* `fp`\n",
                "* `fn`\n",
                "* `tn` \n",
                "\n",
                "You might find it useful to use named parameters here (i.e. you can call `plot_confusion_matrix(tp=X, fp=Y, fn=A, tn=B)` instead of having to get the order of the parameters correct)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q6_confusion_matrix) ###\n",
                "\n",
                "# TODO Compute the four values tp, fp, fn, tn and plot them using plot_confusion_matrix"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Question 7 and 8:** Logistic Regression with L2 regularization\n",
                "\n",
                "One of the challenges of creating features from each word is that there are many more features than observations. It is easy to overfit. We will explore the effect of the regularization in this problem.\n",
                "\n",
                "Now that we have written up all the pieces needed for regularized logistic regression, let's explore the benefits of using **L2 regularization** in analyzing sentiment for product reviews. \n",
                "\n",
                "Like in the last assignment, we will train models with various levels of regularization starting with a small amount and then moving to a  large amount. The code here will have some similarities to the code you did in the last assignment, so you will find that to be a useful reference, but this problem will be slightly more complex since we ask you to compute a few values.\n",
                "\n",
                "This code will be counted as two separate questions since you will be computing slightly different values, but we will ask you to compute both of them in the same starter code to reduce code duplication (since the tasks are very similar). \n",
                "\n",
                "We first outline what you should compute for each question and then show some general implementation notes for both problems below. Your task for this problem is to fill out the code inside the loop to compute the values described below.\n",
                "\n",
                "We recommend focusing on the value you need to compute for Q7 and then once you have that working work on the code you need to compute Q8.\n",
                "\n",
                "### **Question 7:** Coefficient Paths\n",
                "For this question we will ask you to compute the coefficent path for each of the features in the model for various values of the regularization constant.\n",
                "\n",
                "For each regularization strength, train a model using that regularization constant and compute table storing the coefficients of each learned predictor. Store the results in a `DataFrame` named `coef_table`.\n",
                "\n",
                "You should end up with an `DataFrame` with column names as `'coefficients \\[L2=1e-02\\]', ... 'coefficients \\[L2=1e+05\\]'`, and a row for each word in `features`. \n",
                "\n",
                "Before the loop, we set up `coef_table` to have the right rows and columns, but your code will need to fill out the rest.\n",
                "\n",
                "### **Question 8:** Train and Validation Accuracies\n",
                "Similar to Q7, we want you to compute the training and validation accuracy for each learned predictor and store that in a `DataFrame` called `accuracies_table`. \n",
                "\n",
                "You should end up with a `DataFrame` with column names `'l2_penalty', 'train_accuracy', 'validation_accuracy'` and a row for each L2 penalty tried. The L2 penaly should be the number (not the column name from Q7) and the accuracy values should be numbers between 0 and 1 for the appropriate accuracy.\n",
                "\n",
                "For this problem, we recommend the approach used in HW2 to build up a list of dictionaries, and then convert that to a `DataFrame` with the values described.\n",
                "\n",
                "### Implementation Details\n",
                "\n",
                "Some important notes about your implementation:\n",
                "* When constructing a `LogisticRegression` object, make sure to use `random_state=1` to get the same results as us. We also want to avoid having an intercept term in this example, so also pass `fit_intercept=False` when constructing the `LogisticRegression` model.\n",
                "* \u003cspan style=\"color:red\"\u003eWhen constructing the LogisticRegression(...) model, the parameter `C` is the **inverse** of the L2 penalty (1 / L2_penalty). \u003c/span\u003e\n",
                "* Q7: To store the results of your predictor's coefficients, you will need to get the values from the `.coef_` property. Since the code for this is a little complex, we give you this line below (assumes your trained model is stored in a variable called `model`):\n",
                "  ```\n",
                "  coef_table[column_name] = model.coef_[0]\n",
                "  ```\n",
                "\n",
                "  Confusingly this grabs all the coefficients and treats them like a list of numbers rather than the 2D array of rows/columns that scikit-learn originally provides.\n",
                "\n",
                "* It is okay if your code prints `ConvergenceWarnings`. This is something you would want to avoid in practice but is okay in our assignment for simplicity.\n",
                "\n",
                "* We recommend just focusing on Q7 at first and getting the code to set up the coefficients table right. Then once that's working, evaluate the models for Q8.\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q7_q8_train_models) ###\n",
                "\n",
                "# TODO Fill in the loop below\n",
                "\n",
                "# Set up the regularization penalities to try\n",
                "l2_penalties = [0.01, 1, 4, 10, 1e2, 1e3, 1e5]\n",
                "l2_penalty_names = [f'coefficients [L2={l2_penalty:.0e}]' \n",
                "                    for l2_penalty in l2_penalties]\n",
                "\n",
                "# Q7: Add the coefficients to this coef_table for each model\n",
                "coef_table = pd.DataFrame(columns=['word'] + l2_penalty_names)\n",
                "coef_table['word'] = features\n",
                "\n",
                "# Q8: Set up an empty list to store the accuracies (will convert to DataFrame after loop)\n",
                "accuracy_data = []\n",
                "\n",
                "for l2_penalty, l2_penalty_column_name in zip(l2_penalties, l2_penalty_names):\n",
                "    # TODO(Q7 and Q8): Train the model \n",
                "    \n",
                "    # TODO(Q7): Save the coefficients in coef_table\n",
                "\n",
                "    # TODO(Q8): Calculate and save the train and validation accuracies\n",
                "\n",
                "\n",
                "accuracies_table = pd.DataFrame(accuracy_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Look at coef_table\n",
                "coef_table"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Look at accuracies_table\n",
                "accuracies_table"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **Question 9:** Inspect Coefficients\n",
                "\n",
                "We'll now look at the **coefficients** for the model that were trained above. We will create a table of features and learned coefficients associated with each of the different L2 penalty values.\n",
                "\n",
                "Using **the coefficients trained with L2 penalty 1**, find the 5 most positive words (with largest positive coefficients). Save them to `positive_words`. Similarly, find the 5 most negative words (with largest negative coefficients) and save them to `negative_words`. The result should be the `'word'` column for the these rows. \n",
                "\n",
                "To be specific, the type of the value we are looking for is a `Series` in `pandas` which is the type of a single row or column in a `DataFrame`. When you have a `DataFrame`, it is a structure with rows and columns. When you access a single column as in `df[column_name]`, this returns a `Series` representing that one column. \n",
                "\n",
                "This means your result for each one of these variables should be a `Series` of length 5 for the respective words.\n",
                "\n",
                "\n",
                "*Hint:* You can use the `.nlargest()` and `.nsmallest()` method on an DataFrame to find the top `n` rows sorted according to the value of a specified column."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q9_most_positive_negative) ###\n",
                "\n",
                "# TODO Compute words with the 5 largest coefficients and 5 smallest coefficients\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let us observe the effect of increasing L2 penalty on the 10 words just selected. We provide you with a utility function to  plot the coefficient path."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": [
                "def make_coefficient_plot(table, positive_words, negative_words, l2_penalty_list):\n",
                "    def get_cmap_value(cmap, i, total_words):\n",
                "        \"\"\"\n",
                "        Computes a nice scaling of from i=0 to i=total_words - 1\n",
                "        for the given cmap\n",
                "        \"\"\"\n",
                "        return cmap(0.8 * ((i + 1) / (total_words * 1.2) + 0.15))\n",
                "\n",
                "\n",
                "    def plot_coeffs_for_words(ax, words, cmap):\n",
                "        \"\"\"\n",
                "        Given an axes to plot on and a list of words and a cmap,\n",
                "        plots the coefficient paths for each word in words\n",
                "        \"\"\"\n",
                "        words_df = table[table['word'].isin(words)]\n",
                "        words_df = words_df.reset_index(drop=True)  # To make indices sequential\n",
                "\n",
                "        for i, row in words_df.iterrows():\n",
                "            color = get_cmap_value(cmap, i, len(words))\n",
                "            ax.plot(xx, row[row.index != 'word'], '-',\n",
                "                    label=row['word'], linewidth=4.0, color=color)\n",
                "\n",
                "    # Make a canvas to draw on\n",
                "    fig, ax = plt.subplots(1, figsize=(10, 6))\n",
                "   \n",
                "    # Set up the xs to plot and draw a line for y=0\n",
                "    xx = l2_penalty_list\n",
                "    ax.plot(xx, [0.] * len(xx), '--', linewidth=1, color='k')\n",
                "\n",
                "    # Plot the positive and negative coefficient paths\n",
                "    cmap_positive = plt.get_cmap('Reds')\n",
                "    cmap_negative = plt.get_cmap('Blues')\n",
                "    plot_coeffs_for_words(ax, positive_words, cmap_positive)\n",
                "    plot_coeffs_for_words(ax, negative_words, cmap_negative)\n",
                "\n",
                "    # Set up axis labels, scale, and legend  \n",
                "    ax.legend(loc='best', ncol=2, prop={'size':16}, columnspacing=0.5 )\n",
                "    ax.set_title('Coefficient path')\n",
                "    ax.set_xlabel('L2 penalty ($\\lambda$)')\n",
                "    ax.set_ylabel('Coefficient value')\n",
                "    ax.set_xscale('log')\n",
                "\n",
                "\n",
                "make_coefficient_plot(coef_table, positive_words, negative_words, l2_penalty_list=l2_penalties)"
            ]
        }
    ]
}
